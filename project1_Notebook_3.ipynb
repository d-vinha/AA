{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_data = pd.read_csv(r'C:\\Users\\lscon\\Desktop\\AA\\projeto\\the-three-body-problem\\mlNOVA\\X_train.csv')\n",
    "train_data = pd.read_csv(r'C:\\Users\\duart\\OneDrive\\Ambiente_de_Trabalho\\Master_Analysis_Engineering_Big_Data\\23-24\\1st_semester\\AA_ML\\Kaggle_challenges\\3_body_problem\\3_body_problem\\X_train.csv')\n",
    "#train_data = pd.read_csv(r'C:\\Users\\lscon\\Desktop\\AA\\projeto\\the-three-body-problem\\mlNOVA\\X_train.csv')\n",
    "#test_data = pd.read_csv(r'C:/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/3_body_problem/3_body_problem/X_test.csv')\n",
    "\n",
    "print(train_data.shape)\n",
    "np.linalg.matrix_rank(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify faulty rows based on the criterion (all values = 0.0 except for Id)\n",
    "zero_rows = train_data[(train_data.drop('Id', axis=1) == 0).all(axis=1)]\n",
    "\n",
    "# Remove the faulty rows from the DataFrame\n",
    "train_data_preprocessed = train_data[~train_data.index.isin(zero_rows.index)]\n",
    "train_data_preprocessed.reset_index(drop=True, inplace=True)\n",
    "#train_data_preprocessed.to_csv('train_preprocessed.csv', index=False)\n",
    "\n",
    "# np.linalg.matrix_rank(train_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #gives the stats for the preprocessed data (without the rows with zeros)\n",
    "# summary_stats_filtered = train_data_preprocessed.describe(include='all')\n",
    "\n",
    "# #gives the stats for the nonprocessed data\n",
    "# summary_stats = train_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix and plot it as a heatmap\n",
    "corr_matrix = train_data_preprocessed.drop(train_data_preprocessed.columns[13],\n",
    "                                       axis=1).corr()\n",
    "corr_matrix.to_excel('corr_matrix_train_processed.xlsx')\n",
    "\n",
    "plt.figure(figsize=(20, 16), dpi=800)\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.4f',\n",
    "            linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.savefig('Corr_matrix_heatmap.jpg', dpi=800)\n",
    "plt.show()\n",
    "\n",
    "# Create a pairwise scatter plot matrix\n",
    "scatter_matrix = pd.plotting.scatter_matrix(train_data_preprocessed,\n",
    "                                            figsize=(20, 20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairwise plots of correlation between variables\n",
    "\n",
    "rows = 50000\n",
    "x1_rows = x1.head(n=rows)\n",
    "partial_train_data = train_data_preprocessed.drop(columns=['v_x_1','v_x_2','v_y_1', \n",
    "                                                           'v_y_2', 'v_x_3', 'v_y_3']).head(n=rows)\n",
    "\n",
    "\n",
    "_= sns.pairplot(partial_train_data, kind=\"reg\", diag_kind=\"kde\", plot_kws={'line_kws':{'color':'red'}})\n",
    "plt.title('Pairwise plots t vs velocity components')\n",
    "plt.savefig('pairwisetv_50000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature label matrices\n",
    "#we're not going to use the velocity components as features\n",
    "train_data_without_velocity = train_data_preprocessed.drop(columns=['Id','v_x_1','v_x_2','v_y_1', \n",
    "                                                           'v_y_2', 'v_x_3', 'v_y_3'])\n",
    "#divide by simulations\n",
    "list_of_times = [values for values in train_data_without_velocity['t']]\n",
    "time_index_tuples = list(enumerate(list_of_times))\n",
    "zeros_indexes = list(filter(lambda value: value[1] == 0, time_index_tuples))\n",
    "zeros_indexes = [value[0] for value in zeros_indexes] \n",
    "list_of_simulations = []\n",
    "lower_bound = 0\n",
    "for i in range(len(zeros_indexes)-1):\n",
    "    simulation = train_data_without_velocity.iloc[lower_bound:zeros_indexes[i+1]]\n",
    "    list_of_simulations.append(simulation)\n",
    "    lower_bound = zeros_indexes[i+1]\n",
    "from tqdm import tqdm\n",
    "\n",
    "#add label and put the starting position at every row\n",
    "#x1\n",
    "for simulation in tqdm(list_of_simulations):\n",
    "    first_row_values = simulation.head(1)\n",
    "    simulation.loc[:,'x_1_label'] = simulation.loc[:, 'x_1']\n",
    "    simulation.loc[:,'y_1_label'] = simulation.loc[:, 'y_1']\n",
    "    simulation.loc[:,'x_2_label'] = simulation.loc[:, 'x_2']\n",
    "    simulation.loc[:,'y_2_label'] = simulation.loc[:, 'y_2']\n",
    "    simulation.loc[:,'x_3_label'] = simulation.loc[:, 'x_3']\n",
    "    simulation.loc[:,'y_3_label'] = simulation.loc[:, 'y_3']\n",
    "    for index, row in simulation.iterrows():\n",
    "        simulation.at[index, 'x_1'] = first_row_values['x_1']\n",
    "        simulation.at[index, 'y_1'] = first_row_values['y_1']\n",
    "        simulation.at[index, 'x_2'] = first_row_values['x_2']\n",
    "        simulation.at[index, 'y_2'] = first_row_values['y_2']\n",
    "        simulation.at[index, 'x_3'] = first_row_values['x_3']\n",
    "        simulation.at[index, 'y_3'] = first_row_values['y_3']\n",
    "\n",
    "list_of_simulations_copy = list_of_simulations.copy()\n",
    "random.shuffle(list_of_simulations_copy) #we shuffle the data here so we are only shuffling different simulations and not amongst them\n",
    "all_simulations = pd.concat(list_of_simulations_copy, ignore_index=True)\n",
    "all_simulations.to_csv('feature_matrix.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the feature matrix from here after it is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the feature matrix file\n",
    "total_data = np.genfromtxt('feature_matrix.csv', delimiter=',')\n",
    "total_data = total_data[1:] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train, data_temp = train_test_split(total_data, train_size=0.7, shuffle=False)\n",
    "data_vali, data_test = train_test_split(data_temp, test_size=0.5, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the labels from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = data_train[:, :7]  \n",
    "labels_train = data_train[:, 7:] \n",
    "\n",
    "features_vali = data_vali[:, :7]  \n",
    "labels_vali = data_vali[:, 7:] \n",
    "\n",
    "features_test = data_test[:, :7]  \n",
    "labels_test = data_test[:, 7:]\n",
    "\n",
    "np.linalg.matrix_rank(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading a feature matrix and dropping the x1 and y1 columns - splitting in sets and labels from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_matrix_redux = pd.read_csv('feature_matrix.csv')\n",
    "feat_matrix_redux = feat_matrix_redux.drop(columns=['x_1', 'y_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_matrix_redux.to_csv('feat_matrix_redux.csv', index=False)\n",
    "feat_matrix_redux = feat_matrix_redux.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762673, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_redux_train, data_redux_temp = train_test_split(feat_matrix_redux, train_size=0.7, shuffle=False)\n",
    "data_redux_vali, data_redux_test = train_test_split(data_redux_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "data_redux_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762673, 6)\n"
     ]
    }
   ],
   "source": [
    "#split the labels from the features\n",
    "feat_redux_train = data_redux_train[:, :5]  \n",
    "label_redux_train = data_redux_train[:, 5:] \n",
    "\n",
    "feat_redux_vali = data_redux_vali[:, :5]  \n",
    "label_redux_vali = data_redux_vali[:, 5:] \n",
    "\n",
    "feat_redux_test = data_redux_test[:, :5]  \n",
    "label_redux_test = data_redux_test[:, 5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the points (isto e porque acho que era interessante termos no deck of slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial features - let's check the best polynomial + ridge regression\n",
    "# escolher com base no menor MSE com o VALIDATION SET\n",
    "# entretanto descobri que o sklearn tem uma crossvalidation feature que e capaz de ser bem util no calculo do MSE (literalmente transformar\n",
    "# aquilo em duas linhas o que e fixolas)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score\n",
    "\n",
    "# Create a pipeline with PolynomialFeatures take calculates the MSE\n",
    "# We can use it to find the 1st polynomial degree which overfits the data (MSE train =0)\n",
    "# When that happens, we then take that model and do ridge regression for that polynomial features' degree \n",
    "\n",
    "# pipelines = [make_pipeline(StandardScaler(), PolynomialFeatures(6), LinearRegression())] #for degree in range(5, 8)]\n",
    "# # Define the hyperparameters and their respective values to search\n",
    "# alphas = [1e-15, 1e-10, 1e-8, 1e-5, 1e-3, 1e-2, 0.1, 1.0, 10.0, 20, 30, 35, 40, 50, 60, 75, 80, 100]\n",
    "\n",
    "# # pipelines = [make_pipeline(StandardScaler(), PolynomialFeatures(degree), Ridge(alpha)) for degree in range(1, 15) for alpha in alphas]\n",
    "\n",
    "# for pipe in pipelines:\n",
    "#     pipe.fit(features_train, labels_train)\n",
    "#     labels_pred_train = pipe.predict(features_train)\n",
    "#     labels_pred_vali = pipe.predict(features_vali)\n",
    "#     mse_train = mean_squared_error(labels_train, labels_pred_train)\n",
    "#     print(f\"MSE Train:\\t{mse_train}\")\n",
    "#     mse_vali = mean_squared_error(labels_vali, labels_pred_vali)\n",
    "#     print(f\"MSE Vali:\\t{mse_vali}\")\n",
    "#     poly_output_feat = pipe[1].n_output_features_\n",
    "#     print(f\"Polynomial Features:\\t{poly_output_feat}\")\n",
    "\n",
    "# err=[]\n",
    "# for k, pipe in enumerate(pipelines):\n",
    "    \n",
    "#     err.append[mean_squared_error(labels_vali, labels_pred)]\n",
    "\n",
    "\n",
    "# # Calculate RMSE for the best model\n",
    "# labels_train_pred = best_model.predict(features_train)\n",
    "# rmse = sqrt(mean_squared_error(labels_train, labels_train_pred))\n",
    "# print(\"Root Mean Square Error (RMSE) for the best model:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depois no final grafico para o melhor claro e calculamos sqrt(mse) para vermos o quao off estamos - test set\n",
    "#depois seria implementar isto tudo para as outras matrizes que faltam\n",
    "#sugeria depois transformar isto do modelo numa funcao para ser mais simples e nao repetirmos codigo\n",
    "# o mesmo poderia ser feito para a criaçao das matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Vali:\t1.1511185968083442\n",
      "RMSE Test:\t1.5047333023544216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline_redux_ridge = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('polynomial_features', PolynomialFeatures(6)),\n",
    "    ('regressor', Ridge(100))\n",
    "])\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline_redux_ridge.fit(feat_redux_train, label_redux_train)\n",
    "\n",
    "# Make predictions on the validation data using the pipeline model\n",
    "labels_pred_vali_redux = pipeline_redux_ridge.predict(feat_redux_vali)\n",
    "\n",
    "# Evaluate the performance of the pipeline model on the validation data\n",
    "mse_vali = mean_squared_error(label_redux_vali, labels_pred_vali_redux, squared=False)\n",
    "print(f\"RMSE Vali:\\t{mse_vali}\")\n",
    "\n",
    "# Make predictions on the test data using the pipeline model\n",
    "labels_pred_test_redux = pipeline_redux_ridge.predict(feat_redux_test)\n",
    "\n",
    "# Evaluate the performance of the pipeline model on the test data\n",
    "mse_test = mean_squared_error(label_redux_test, labels_pred_test_redux, squared=False)\n",
    "print(f\"RMSE Test:\\t{mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Ridge and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the range of alpha values to evaluate\n",
    "alpha_values = np.linspace(0.1, 10, 10)\n",
    "alphas = [0.5, 0.9]\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('polynomial_features', PolynomialFeatures(5)),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "# Create a grid search\n",
    "grid_search = GridSearchCV(pipeline, {'regressor__alpha': alphas}, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation data using the best model\n",
    "labels_pred_vali = best_model.predict(features_vali)\n",
    "\n",
    "# Evaluate the performance of the best model on the validation data\n",
    "mse_vali = mean_squared_error(labels_vali, labels_pred_vali, squared=False)\n",
    "print(f\"RMSE Vali:\\t{mse_vali}\")\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "labels_pred_test = best_model.predict(features_test)\n",
    "\n",
    "# Evaluate the performance of the best model on the test data\n",
    "mse_test = mean_squared_error(labels_test, labels_pred_test, squared=False)\n",
    "print(f\"RMSE Test:\\t{mse_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Ridge Regression without GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\duart\\OneDrive\\Ambiente_de_Trabalho\\Master_Analysis_Engineering_Big_Data\\23-24\\1st_semester\\AA_ML\\Kaggle_challenges\\AA-3_body_problem\\AA\\project1_Notebook_2.ipynb Cell 25\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression, Ridge\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Create a pipeline object\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pipeline_ridge \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mscaler\u001b[39m\u001b[39m'\u001b[39m, StandardScaler()),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mpolynomial_features\u001b[39m\u001b[39m'\u001b[39m, PolynomialFeatures(\u001b[39m6\u001b[39m)),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mregressor\u001b[39m\u001b[39m'\u001b[39m, Ridge(\u001b[39m100\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Fit the pipeline model to the training data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m pipeline_ridge\u001b[39m.\u001b[39mfit(features_train, labels_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline_ridge = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('polynomial_features', PolynomialFeatures(6)),\n",
    "    ('regressor', Ridge(100))\n",
    "])\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline_ridge.fit(features_train, labels_train)\n",
    "\n",
    "# Make predictions on the validation data using the pipeline model\n",
    "labels_pred_vali = pipeline_ridge.predict(features_vali)\n",
    "\n",
    "# Evaluate the performance of the pipeline model on the validation data\n",
    "mse_vali = mean_squared_error(labels_vali, labels_pred_vali, squared=False)\n",
    "print(f\"RMSE Vali:\\t{mse_vali}\")\n",
    "\n",
    "# Make predictions on the test data using the pipeline model\n",
    "labels_pred_test = pipeline_ridge.predict(features_test)\n",
    "\n",
    "# Evaluate the performance of the pipeline model on the test data\n",
    "mse_test = mean_squared_error(labels_test, labels_pred_test, squared=False)\n",
    "print(f\"RMSE Test:\\t{mse_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with LASSO regression without GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Vali:\t1.33016875390813\n",
      "RMSE Test:\t1.1591774716969916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline_lasso = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('polynomial_features', PolynomialFeatures(6)),\n",
    "    ('regressor', Lasso(alpha=0.1))\n",
    "])\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline_lasso.fit(features_train, labels_train)\n",
    "\n",
    "# Make predictions on the validation data using the pipeline model\n",
    "labels_pred_vali = pipeline_lasso.predict(features_vali)\n",
    "\n",
    "# Evaluate the performance of the pipeline model on the validation data\n",
    "mse_vali = mean_squared_error(labels_vali, labels_pred_vali, squared=False)\n",
    "print(f\"RMSE Vali:\\t{mse_vali}\")\n",
    "\n",
    "# Make predictions on the test data using the pipeline model\n",
    "labels_pred_test = pipeline_lasso.predict(features_test)\n",
    "\n",
    "# Evaluate the performance of the pipeline model on the test data\n",
    "mse_test = mean_squared_error(labels_test, labels_pred_test, squared=False)\n",
    "print(f\"RMSE Test:\\t{mse_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying on real world data - Creating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    4\n",
      "Name: Id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the real-world dataset into a Pandas DataFrame and drop the Id column - RUN ONLY ONCE PER TRIALRUN\n",
    "X_realworld = pd.read_csv(r'C:/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/3_body_problem/3_body_problem/X_test.csv')\n",
    "id_column = X_realworld['Id']\n",
    "print(id_column.head())\n",
    "X_realworld.drop('Id', axis=1, inplace=True)\n",
    "X_realworld.to_csv('X_realworld.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGE TO THE FULL NAME OF THE PIPELINE YOU WANT TO CALL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\duart\\OneDrive\\Ambiente_de_Trabalho\\Master_Analysis_Engineering_Big_Data\\23-24\\1st_semester\\AA_ML\\Kaggle_challenges\\AA-3_body_problem\\AA\\project1_Notebook_2.ipynb Cell 31\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# # Preprocess the data using the pipeline\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# # Preprocess the real-world data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# X_realworld_processed = pipeline.named_steps['scaler'].transform(X_realworld)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Make predictions on the preprocessed real-world data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m predictions_realworld \u001b[39m=\u001b[39m pipeline_ridge\u001b[39m.\u001b[39mpredict(X_realworld)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Create a new Pandas DataFrame with the predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/AA-3_body_problem/AA/project1_Notebook_2.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df_predictions \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(predictions_realworld)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline_ridge' is not defined"
     ]
    }
   ],
   "source": [
    "# # Preprocess the data using the pipeline\n",
    "# # Preprocess the real-world data\n",
    "# X_realworld_processed = pipeline.named_steps['scaler'].transform(X_realworld)\n",
    "# X_realworld_processed = pipeline.named_steps['polynomial_features'].transform(X_realworld_processed)\n",
    "\n",
    "\n",
    "# Make predictions on the preprocessed real-world data\n",
    "predictions_realworld = pipeline_ridge.predict(X_realworld)\n",
    "\n",
    "\n",
    "# Create a new Pandas DataFrame with the predictions\n",
    "df_predictions = pd.DataFrame(predictions_realworld)\n",
    "df_predictions.insert(loc=0, column='Id', value = id_column)\n",
    "df_predictions.columns=['Id', 'x_1', 'y_1', 'x_2', 'y_2', 'x_3', 'y_3']\n",
    "\n",
    "# Submit the Pandas DataFrame to the challenge creator\n",
    "df_predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1041621, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_predictions.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
